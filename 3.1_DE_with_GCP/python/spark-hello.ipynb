{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c502c9",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "\n",
    "Spark based on Java, so you need to install Java before running spark. Download and install Java [from here](https://docs.aws.amazon.com/corretto/index.html). Spark currently support java 8 or 11. Please see the compatibility on [Spark documentation](https://spark.apache.org/docs/latest/)\n",
    "\n",
    "Then, install pyspark using pip.\n",
    "  \n",
    "`#> pip install pyspark`\n",
    "\n",
    "For this hello spark sample, it will read data set is from a local file (`spark_people.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19797ca7",
   "metadata": {},
   "source": [
    "Since we're using Spark locally we already have both a SparkContext and a SparkSession running.  \n",
    "We can update some of the parameters, such our application's name. \n",
    "Let's just call it *Hello Spark*.\n",
    "  \n",
    "This might take some time for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Hello Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc5ef",
   "metadata": {},
   "source": [
    "See the spark session. We can also open the link to spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f5916",
   "metadata": {},
   "source": [
    "## Pyspark CSV Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad21e6",
   "metadata": {},
   "source": [
    "Read from csv file, where first row is header row. The result is spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985efbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/spark_people.csv\"\n",
    "people_data = spark.read.csv(path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613550e",
   "metadata": {},
   "source": [
    "Spark schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cc590",
   "metadata": {},
   "source": [
    "Show top 10 data, without truncating long value (using `False` as parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c706ad2",
   "metadata": {},
   "source": [
    "Select particular columns, then show top 10 data, without truncating long value (using `False` as parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data.select(\"email\", \"full_name\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54573b",
   "metadata": {},
   "source": [
    "Take and iterate the last 12 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0541307",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in people_data.tail(12):\n",
    "    print(\"id_number is {}, full_name is {}, email is {}, and address is {}\".format(\n",
    "        row.id_number, row.full_name, row[\"email\"], row[\"address\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce00980",
   "metadata": {},
   "source": [
    "## Pyspark JSONL Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c17af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/spark_people.jsonl\"\n",
    "people_data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d39b5",
   "metadata": {},
   "source": [
    "See the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56949d40",
   "metadata": {},
   "source": [
    "Show the first 10 records, without truncating long value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5674b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec5a07",
   "metadata": {},
   "source": [
    "Iterate the first 8 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ea786",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in people_data.take(8):\n",
    "    print(\"id_number is {}, full_name is {}, email is {}, and address is {}\".format(\n",
    "        row.id_number, row.full_name, row[\"email\"], row[\"address\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd1b78",
   "metadata": {},
   "source": [
    "## Pyspark JSON Array Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4da8a",
   "metadata": {},
   "source": [
    "If the file is json array, not jsonl, then add `multiLine=True` while reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39909065",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/spark_people.json\"\n",
    "people_json_data = spark.read.json(path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_json_data.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in people_json_data.take(8):\n",
    "    print(\"id_number is {}, full_name is {}, email is {}, and address is {}\".format(\n",
    "        row.id_number, row.full_name, row[\"email\"], row[\"address\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff0181",
   "metadata": {},
   "source": [
    "The complete pyspark API reference [available here.](http://spark.apache.org/docs/latest/api/python/reference/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
