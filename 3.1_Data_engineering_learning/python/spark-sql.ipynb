{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae10115",
   "metadata": {},
   "source": [
    "# Spark Basic Syntax Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required if at some point you got \n",
    "# 'java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182e163",
   "metadata": {},
   "source": [
    "Open spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33941a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c04d7",
   "metadata": {},
   "source": [
    "Build dataframe from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7935d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/spark_loan.jsonl\"\n",
    "loan_df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb8604",
   "metadata": {},
   "source": [
    "## Build View for Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eed9f",
   "metadata": {},
   "source": [
    "Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.createOrReplaceTempView(\"loan_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a3d4e",
   "metadata": {},
   "source": [
    "Count the data frame rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(loan_id) FROM loan_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f53be",
   "metadata": {},
   "source": [
    "These two lines is to eliminite line break for text wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2478748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1b14f",
   "metadata": {},
   "source": [
    "Select all fields. You can use multi line for spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "      FROM loan_table\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c0766",
   "metadata": {},
   "source": [
    "Or, just select few fields to describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT full_name, loan_amount\n",
    "      FROM loan_table\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fed883",
   "metadata": {},
   "source": [
    "Drop duplicates, and show the distinct records, ordered by certain field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT payment_period, loan_period_weeks\n",
    "      FROM loan_table\n",
    "     ORDER BY 1, 2\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e0850",
   "metadata": {},
   "source": [
    "Collect all data into local python memory. Careful, if the data is very large, this might trigger an out-of-memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM loan_table\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7faae58",
   "metadata": {},
   "source": [
    "Alternatively, just take the first *n* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM loan_table LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c4422",
   "metadata": {},
   "source": [
    "The query result is spark dataframe, and can be processed as regular spark dataframe, including conversion to pandas dataframe.\n",
    "\n",
    "**Note** : `toPandas` will collects all data into the local python, which can cause an out-of-memory-error when the data is too large to fit into one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d39f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_from_sql = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "      FROM loan_table \n",
    "     LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "pandas_loan_df = loan_df_from_sql.toPandas()\n",
    "pandas_loan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334df1b",
   "metadata": {},
   "source": [
    "Filtering data, show only loan amount between 500-700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65345323",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_from_sql = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "      FROM loan_table \n",
    "     WHERE loan_amount >= 500 AND loan_amount <= 700\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52d5b2",
   "metadata": {},
   "source": [
    "Sort by multiple columns & ascending / descending. For example, sort by `loan_approved_date` (ascending), then by `loan_amount` (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53950aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_from_sql = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "      FROM loan_table \n",
    "     WHERE loan_amount < 500 OR loan_amount > 700\n",
    "  ORDER BY loan_approved_date, loan_amount DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ebccf",
   "metadata": {},
   "source": [
    "## Grouping & Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e363c7",
   "metadata": {},
   "source": [
    "Aggregation using SQL-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_from_sql = spark.sql(\"\"\"\n",
    "    SELECT loan_rating, COUNT(loan_id) AS count_loan_rating\n",
    "      FROM loan_table\n",
    "  GROUP BY loan_rating\n",
    "  ORDER BY loan_rating\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8bcb9",
   "metadata": {},
   "source": [
    "Nested function like this (rounding the average to 2 decimals) also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_from_sql = spark.sql(\"\"\"\n",
    "    SELECT payment_period, ROUND(AVG(loan_amount), 2) AS avg_amount\n",
    "      FROM loan_table\n",
    "  GROUP BY payment_period\n",
    "  ORDER BY payment_period\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab589334",
   "metadata": {},
   "source": [
    "## UDF (User-Defined-Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6a663",
   "metadata": {},
   "source": [
    "Calculate and create new column `loan_end_date` based on user-defined-function \n",
    "(`udf`) with formula \n",
    "\n",
    "`loan_end_date = loan_approved_date + loan_period_weeks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loan_end_date = udf(lambda x, y: (datetime.fromisoformat(x) + timedelta(weeks=y)).strftime('%Y-%m-%d') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b92d4a",
   "metadata": {},
   "source": [
    "Need to register the UDF to be used by Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"get_loan_end_date\", get_loan_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcb435",
   "metadata": {},
   "source": [
    "Now, we can use the registered udf as function on Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT loan_id, loan_approved_date, loan_period_weeks, \n",
    "           get_loan_end_date(loan_approved_date, loan_period_weeks) AS loan_end_date\n",
    "      FROM loan_table\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a839f1",
   "metadata": {},
   "source": [
    "UDF using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7297b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_repeat(str):\n",
    "    if random.choice([True, False]):\n",
    "        return str + \" & \" + str\n",
    "    else:\n",
    "        return str + \" & \" + str + \" & \" + str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6152dc1",
   "metadata": {},
   "source": [
    "Register the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_repeat_udf = spark.udf.register(\"just_repeat\", just_repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb6d25",
   "metadata": {},
   "source": [
    "Use the UDF on Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20319580",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT full_name, just_repeat(full_name) AS just_repeating_column\n",
    "      FROM loan_table\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1d585",
   "metadata": {},
   "source": [
    "## SQL Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb8969",
   "metadata": {},
   "source": [
    "Spark SQL function reference [available here](https://spark.apache.org/docs/latest/api/sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
